<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Using Fast Weights to Attend to the Recent Past</title>
  <meta name="description" content="论文相关信息论文nips链接">

  <link rel="stylesheet" href="/mark/css/main.css">
  <link rel="stylesheet" href="/mark/css/pygments.css">
  <link rel="shortcut icon" href="/mark/favicon.ico">
  <link rel="canonical" href="http://localhost:4000/mark/paper/2017/11/07/using-fast-weights-to-attend-to-the-recent-past.html">
  <link rel="alternate" type="application/rss+xml" title="Blog" href="http://localhost:4000/mark/feed.xml" />
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/mark/">Blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/mark/about/">About</a>
          
        
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Using Fast Weights to Attend to the Recent Past</h1>
    <p class="post-meta">Nov 7, 2017</p>
  </header>

  <article class="post-content">
    <h2>论文相关信息</h2>

<p><a href="http://papers.nips.cc/paper/by-source-2016-2143"><strong>论文nips链接</strong></a></p>

<p><a href="http://papers.nips.cc/paper/6057-using-fast-weights-to-attend-to-the-recent-past.pdf"><strong>论文pdf链接</strong></a></p>

<h2>相关知识</h2>

<p><strong>记忆分类</strong></p>

<ol>
<li>瞬时记忆，又称为感觉记忆或者感觉登记——也就是你现在看到、听到感觉到的一切信息在人脑中的反应。</li>
<li>短时记忆——请你现在回忆看这个答案前你再看什么？这就是短时记忆，一般持续15~30秒。（没有复述的情况下）</li>
<li>长时记忆——也就是一分钟以上的记忆，最长可以达到终身。</li>
</ol>

<p>以上定义来自于《普通心理学》北京师范出版社·彭聃龄</p>

<p><strong>循环神经网络</strong></p>

<p><img src="/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/RNN.png" alt="RNN"></p>

<p><strong>长短时记忆网络</strong></p>

<p><img src="/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/LSTM.png" alt="LSTM"></p>

<h2>论文理解</h2>

<p>普通的循环神经网络有两种记忆类型，即短期记忆和长期记忆。隐藏层的神经元负责保持短期记忆，每次迭代都更新，记忆容量为\(O(H)\)，其中\(H\)是隐藏层神经元的个数。权重向量保持着长期记忆，记忆容量为\(O(H ^{2})+O(IH)+O(HO)\)，其中\(I\)和\(O\)是输入和输出单元的个数。LSTM增加了各种门和状态信息，使得记忆可以保存的更久，但是它的短期记忆容量依旧为\(O(H)\)。从生理学上来说，人的记忆并不是存储在神经元中。生理学上表明，大脑的短期记忆的某一种实现方式是：通过突出末梢的\([Ca ^{2+}]]\)去极化建立短期记忆，而又通过递质的损耗来遗忘。这也就是说大脑的短期记忆并不是存储于神经元中的，而是存储在突触的变化中。这样短期记忆的容量就可以达到\(O(H ^{2})\)。</p>

<h3>快速联想记忆</h3>

<p>1970-1980年的主流思想是，记忆不是以简单的拷贝神经活动的模式存储的，而是在需要时从存有信息的权重中重建的。\(O(N ^{2})\)个权重至多存储\(O(N)\)个实值向量，每个向量有\(O(N)\)个分量。至于实际存储容量能够达到什么程度则取决于我们采用的存储规则。由于我们觉得一个简单的，无迭代的存规则比最大化容量要重要一些，所以我们采用外积规则来存储隐藏快速衰减的神经向量。</p>

<h4>快速联想记忆的优势：</h4>

<ol>
<li>对于像神经图灵机，神经堆栈，记忆网络这些模型暂时没有生理学的依据，但是大脑实现快速联想记忆却是有依据的即通过突触的变化。</li>
<li>在快速联想记忆中，我们不必决定什么时候读记忆什么时候写记忆，快速联想记忆时时刻刻都在更新。每当输入变化时，都可以通过模型反应到隐藏状态中去。</li>
</ol>

<h4>模型</h4>

<p><img src="/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/fast-associative-memory-model.png" alt="fast-associative-memory-model"></p>

<p><strong>迭代公式</strong>
$$A \left ( t \right ) = \lambda  A \left ( t-1 \right ) + \eta h \left ( t \right ) h \left ( t \right ) ^{T}$$</p>

<p>$$(Assuming\ A\ is\ 0\ at\ the\ beginning\ of\ the\ sequence)$$</p>

<p>$$\Longrightarrow A \left ( t \right ) = \eta  \sum _{\tau = 1} ^{\tau = t} \lambda ^{t- \tau} h \left ( \tau \right ) h \left ( \tau \right ) ^{T}$$</p>

<p>$$h _{0} \left ( t+1 \right ) = f \left ( Wh \left ( t \right ) + Cx \left ( t \right ) \right )$$</p>

<p>$$h _{S} \left ( t+1 \right ) = f \left ( \left [ Wh \left ( t \right ) + Cx \left ( t \right ) \right ] + A \left ( t \right ) h _{S-1} \left ( t+1 \right ) \right ) $$</p>

<p>$$h \left ( t+1 \right ) = h _{S} \left ( t+1 \right )$$</p>

<p>$$A \left ( t \right ) h _{s-1} \left ( t+1 \right ) = \eta  \sum _{\tau = 1} ^{\tau = t} \lambda ^{t- \tau} h \left ( \tau \right ) \left [ h \left ( \tau \right ) ^{T} h _{s-1} \left ( t+1 \right ) \right ]$$</p>

<p>其中\(A \left ( t \right )\)不需要训练，是随着时间和输入更新的
<strong>Layer normalized fast weight</strong>
快速联想记忆网络的一个可能潜在的问题是两个隐藏向量的标量积可能会消失或者爆炸，而最近提出的layer normalization在处理这样的问题时表现出色，并且可以减少训练时间，所以我们在快速联想记忆网络中使用这一技术如下：
$$h _{S} \left ( t+1 \right ) = f \left (LN \left [ Wh \left ( t \right ) + Cx \left ( t \right ) + A \left ( t \right ) h _{S-1} \left ( t+1 \right ) \right ] \right ) $$
其中\(LN\left [\cdot \right ] \)代表layer normalization。</p>

<h3>实验结果</h3>

<ol>
<li><p>联想检索</p>

<p><img src="/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/associative-retrieval.png" alt="associative-retrieval">
<img src="/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/associative-retrieval-result.png" alt="associative-retrieva-resultl"></p></li>
<li><p>在视觉注意力模型中综合瞥见</p>

<p>视觉注意力模型能将注意力集中在图像的某些重要的部分，而忽略图像的混乱的背景，该模型不仅决定它下一步要看什么，而且要把所看的东西都记住以便进行正确的图像分类。视觉注意力模型能在大量图像输入中找出多个图像并进行正确分类。但是它的瞥见策略过于简单，只能使用单一尺度的瞥见，并且扫描图像的方式也过于死板。相对来说人眼的运动就复杂多了，人眼可以看同一东西的不同尺度，并且能够将看到的东西给予不同的重要程度划分。但是这要求我们能具有短期的记忆能记住我们在刚开始的一系列瞥见中发现了什么。
典型的视觉注意力模型，比较复杂且效果的方差比较大，为了证明快速联想记忆对视觉注意力模型有助益，我们让提前定义好了视觉注意力模型的控制信号，所以模型不需要学习下一步要看什么，这样可以更加明显的看出快速权重的作用。
<img src="/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/integrating-glimpses-in-visual-attention-models-result.png" alt="integrating-glimpses-in-visual-attention-models-result"></p></li>
<li><p>面部表情识别
<img src="/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/facial-expression-recognition-result.png" alt="facial-expression-recognition-result"></p></li>
<li><p>具有记忆的智能体
<img src="/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/agents-with-memory-result.png" alt="agents-with-memory-result"></p></li>
</ol>

<h3>扩展阅读</h3>

<ul>
<li><p>A-LSTM</p></li>
<li><p>visual attention models(模型)</p></li>
<li><p>Negative log likelihood（评价指标）</p></li>
<li><p>batch normalization(归一化)</p></li>
<li><p>layer normalization(归一化)</p></li>
<li><p>Asynchronous advantage actor-critic method(训练方法)</p></li>
</ul>

<h2>作者信息</h2>

<p><strong>Jimmy Ba:</strong>University of Toronto</p>

<ul>
<li><p>Using Fast Weights to Attend to the Recent Past (2016)</p></li>
<li><p>Learning Wake-Sleep Recurrent Attention Models (2015)</p></li>
<li><p>Do Deep Nets Really Need to be Deep? (2014)</p></li>
<li><p>Adaptive dropout for training deep neural networks (2013)</p></li>
</ul>

<p><strong>Geoffrey Hinton:</strong>University of Toronto and Google Brain</p>

<p>计算机学家和心理学家，反向传播算法和对比散度算法的发明人之一，深度学习的积极推动者。盖茨比计算神经科学中心的创始人，多伦多大学计算机系教授。对神经网络的研究的其它贡献包括Boltzmann机器，分布式表征，时滞神经网络，混合专家、变分学习，专家产品和深信网络。研究兴趣是用丰富传感器输入的非监督式多层神经网络学习程序。可以说是Deep Learning学派的开山祖师爷。</p>

<ul>
<li><p>Attend, Infer, Repeat: Fast Scene Understanding with Generative Models (2016)</p></li>
<li><p>A Better Way to Pretrain Deep Boltzmann Machines (2012)</p></li>
<li><p>ImageNet Classification with Deep Convolutional Neural Networks (2012)</p></li>
<li><p>Gated Softmax Classification (2010)</p></li>
<li><p>Generating more realistic images using gated MRF&#39;s (2010)</p></li>
<li><p>Learning to combine foveal glimpses with a third-order Boltzmann machine (2010)</p></li>
<li><p>Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine (2010)</p></li>
</ul>

<p><strong>Volodymyr Mnih:</strong>Google DeepMind</p>

<ul>
<li><p>Learning values across many orders of magnitude (2016)</p></li>
<li><p>Strategic Attentive Writer for Learning Macro-Actions (2016)</p></li>
<li><p>Recurrent Models of Visual Attention (2014)</p></li>
<li><p>Generating more realistic images using gated MRF&#39;s (2010)</p></li>
</ul>

<p><strong>Joel Leibo:</strong>Google DeepMind</p>

<ul>
<li><p>Learning invariant representations and applications to face verification (2013)</p></li>
<li><p>Why The Brain Separates Face Recognition From Object Recognition (2011)</p></li>
</ul>

<p><strong>Catalin Ionescu:</strong>Google DeepMind</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Blog</li>
          <li><a href="mailto:wdkrgst@gmail.com">wdkrgst@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          

          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text"></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
