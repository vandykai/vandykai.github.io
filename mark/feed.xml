<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog</title>
    <description></description>
    <link>http://localhost:4000/mark/</link>
    <atom:link href="http://localhost:4000/mark/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 24 Aug 2018 18:12:48 +0800</pubDate>
    <lastBuildDate>Fri, 24 Aug 2018 18:12:48 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>Learning Deep Structured Semantic Models for Web Search using Clickthrough Data (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2018-08-11-learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/2018-08-11-learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data.pdf#page=1&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 23 May 2018 21:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/paper/2018/05/23/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/paper/2018/05/23/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data.html</guid>
        
        <category>2014</category>
        
        
        <category>Paper</category>
        
      </item>
    
      <item>
        <title>Neural Networks and Physical Systems with Emergent Collective Computational Abilities (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2018-07-26-neural-networks-and-physical-systems-with-emergent-collective-computational-abilities/2018-07-26-neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.pdf#page=1&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 23 May 2018 21:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/paper/2018/05/23/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/paper/2018/05/23/neural-networks-and-physical-systems-with-emergent-collective-computational-abilities.html</guid>
        
        <category>2014</category>
        
        
        <category>Paper</category>
        
      </item>
    
      <item>
        <title>Neural Turing Machines (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2018-05-23-neural-turing-machines/2018-05-23-neural-turing-machines.pdf#page=1&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 23 May 2018 21:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/paper/2018/05/23/neural-turing-machines.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/paper/2018/05/23/neural-turing-machines.html</guid>
        
        <category>2014</category>
        
        
        <category>Paper</category>
        
      </item>
    
      <item>
        <title>Machine Learning And Its Application In Image Processing (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2017-06-29-machine-learning-and-its-application-in-image-processing/2017-06-29-machine-learning-and-its-application-in-image-processing.pdf#page=1&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 23 May 2018 21:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/ai/2018/05/23/machine-learning-and-its-application-in-image-processing.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/ai/2018/05/23/machine-learning-and-its-application-in-image-processing.html</guid>
        
        <category>AI</category>
        
        <category>image-processing</category>
        
        
        <category>AI</category>
        
      </item>
    
      <item>
        <title>Ruby On Rails</title>
        <description>&lt;h2&gt;安装&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;安装ruby&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装sqlite3&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install sqlite3
sudo apt-get install libsqlite3-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装rails&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;gem install rails
rails --version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用rails创建博客&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;rails new blog
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;启动web服务器&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;cd blog
bin/rails server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;错误及处理&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:84:in `rescue in block (2 levels) in require&amp;#39;: There was an error while trying to load the gem &amp;#39;uglifier&amp;#39;. (Bundler::GemRequireError)
Gem Load Error is: Could not find a JavaScript runtime. See https://github.com/rails/execjs for a list of available runtimes.
Backtrace for gem load error is:
/var/lib/gems/2.3.0/gems/execjs-2.7.0/lib/execjs/runtimes.rb:58:in `autodetect&amp;#39;
/var/lib/gems/2.3.0/gems/execjs-2.7.0/lib/execjs.rb:5:in `&amp;lt;module:ExecJS&amp;gt;&amp;#39;
/var/lib/gems/2.3.0/gems/execjs-2.7.0/lib/execjs.rb:4:in `&amp;lt;top (required)&amp;gt;&amp;#39;
/var/lib/gems/2.3.0/gems/uglifier-4.1.1/lib/uglifier.rb:5:in `require&amp;#39;
/var/lib/gems/2.3.0/gems/uglifier-4.1.1/lib/uglifier.rb:5:in `&amp;lt;top (required)&amp;gt;&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:81:in `require&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:81:in `block (2 levels) in require&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:76:in `each&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:76:in `block in require&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:65:in `each&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:65:in `require&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler.rb:114:in `require&amp;#39;
/home/ez/blog/config/application.rb:7:in `&amp;lt;top (required)&amp;gt;&amp;#39;
/var/lib/gems/2.3.0/gems/railties-5.1.4/lib/rails/commands/server/server_command.rb:133:in `require&amp;#39;
...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;原因：没有JavaScript runtime
解决方法：sudo apt-get install nodejs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        <pubDate>Sat, 23 Dec 2017 20:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/web/2017/12/23/begin-ruby-on-rails.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/web/2017/12/23/begin-ruby-on-rails.html</guid>
        
        <category>web ruby ruby-on-rails</category>
        
        
        <category>Web</category>
        
      </item>
    
      <item>
        <title>Playing Atari with Deep Reinforcement Learning (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2017-12-23-playing-atari-with-deep-reinforcement-learning/2017-12-23-playing-atari-with-deep-reinforcement-learning.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Dec 2017 20:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/paper/2017/12/23/playing-atari-with-deep-reinforcement-learning.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/paper/2017/12/23/playing-atari-with-deep-reinforcement-learning.html</guid>
        
        <category>nips</category>
        
        <category>2013</category>
        
        
        <category>Paper</category>
        
      </item>
    
      <item>
        <title>Reinforcement Learning - Markov Decision Process  (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2017-12-23-markov-decision-process/2017-12-23-markov-decision-process.pdf#page=1&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Dec 2017 20:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/reinforcement-learning/2017/12/23/markov-decision-process.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/reinforcement-learning/2017/12/23/markov-decision-process.html</guid>
        
        <category>MDP</category>
        
        
        <category>Reinforcement-Learning</category>
        
      </item>
    
      <item>
        <title>Using Fast Weights to Attend to the Recent Past</title>
        <description>&lt;h2&gt;论文相关信息&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/by-source-2016-2143&quot;&gt;&lt;strong&gt;论文nips链接&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6057-using-fast-weights-to-attend-to-the-recent-past.pdf&quot;&gt;&lt;strong&gt;论文pdf链接&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;相关知识&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;记忆分类&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;瞬时记忆，又称为感觉记忆或者感觉登记——也就是你现在看到、听到感觉到的一切信息在人脑中的反应。&lt;/li&gt;
&lt;li&gt;短时记忆——请你现在回忆看这个答案前你再看什么？这就是短时记忆，一般持续15~30秒。（没有复述的情况下）&lt;/li&gt;
&lt;li&gt;长时记忆——也就是一分钟以上的记忆，最长可以达到终身。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上定义来自于《普通心理学》北京师范出版社·彭聃龄&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;循环神经网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/RNN.png&quot; alt=&quot;RNN&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;长短时记忆网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/LSTM.png&quot; alt=&quot;LSTM&quot;&gt;&lt;/p&gt;

&lt;h2&gt;论文理解&lt;/h2&gt;

&lt;p&gt;普通的循环神经网络有两种记忆类型，即短期记忆和长期记忆。隐藏层的神经元负责保持短期记忆，每次迭代都更新，记忆容量为\(O(H)\)，其中\(H\)是隐藏层神经元的个数。权重向量保持着长期记忆，记忆容量为\(O(H ^{2})+O(IH)+O(HO)\)，其中\(I\)和\(O\)是输入和输出单元的个数。LSTM增加了各种门和状态信息，使得记忆可以保存的更久，但是它的短期记忆容量依旧为\(O(H)\)。从生理学上来说，人的记忆并不是存储在神经元中。生理学上表明，大脑的短期记忆的某一种实现方式是：通过突出末梢的\([Ca ^{2+}]]\)去极化建立短期记忆，而又通过递质的损耗来遗忘。这也就是说大脑的短期记忆并不是存储于神经元中的，而是存储在突触的变化中。这样短期记忆的容量就可以达到\(O(H ^{2})\)。&lt;/p&gt;

&lt;h3&gt;快速联想记忆&lt;/h3&gt;

&lt;p&gt;1970-1980年的主流思想是，记忆不是以简单的拷贝神经活动的模式存储的，而是在需要时从存有信息的权重中重建的。\(O(N ^{2})\)个权重至多存储\(O(N)\)个实值向量，每个向量有\(O(N)\)个分量。至于实际存储容量能够达到什么程度则取决于我们采用的存储规则。由于我们觉得一个简单的，无迭代的存规则比最大化容量要重要一些，所以我们采用外积规则来存储隐藏快速衰减的神经向量。&lt;/p&gt;

&lt;h4&gt;快速联想记忆的优势：&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;对于像神经图灵机，神经堆栈，记忆网络这些模型暂时没有生理学的依据，但是大脑实现快速联想记忆却是有依据的即通过突触的变化。&lt;/li&gt;
&lt;li&gt;在快速联想记忆中，我们不必决定什么时候读记忆什么时候写记忆，快速联想记忆时时刻刻都在更新。每当输入变化时，都可以通过模型反应到隐藏状态中去。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;模型&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/fast-associative-memory-model.png&quot; alt=&quot;fast-associative-memory-model&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;迭代公式&lt;/strong&gt;
$$A \left ( t \right ) = \lambda  A \left ( t-1 \right ) + \eta h \left ( t \right ) h \left ( t \right ) ^{T}$$&lt;/p&gt;

&lt;p&gt;$$(Assuming\ A\ is\ 0\ at\ the\ beginning\ of\ the\ sequence)$$&lt;/p&gt;

&lt;p&gt;$$\Longrightarrow A \left ( t \right ) = \eta  \sum _{\tau = 1} ^{\tau = t} \lambda ^{t- \tau} h \left ( \tau \right ) h \left ( \tau \right ) ^{T}$$&lt;/p&gt;

&lt;p&gt;$$h _{0} \left ( t+1 \right ) = f \left ( Wh \left ( t \right ) + Cx \left ( t \right ) \right )$$&lt;/p&gt;

&lt;p&gt;$$h _{S} \left ( t+1 \right ) = f \left ( \left [ Wh \left ( t \right ) + Cx \left ( t \right ) \right ] + A \left ( t \right ) h _{S-1} \left ( t+1 \right ) \right ) $$&lt;/p&gt;

&lt;p&gt;$$h \left ( t+1 \right ) = h _{S} \left ( t+1 \right )$$&lt;/p&gt;

&lt;p&gt;$$A \left ( t \right ) h _{s-1} \left ( t+1 \right ) = \eta  \sum _{\tau = 1} ^{\tau = t} \lambda ^{t- \tau} h \left ( \tau \right ) \left [ h \left ( \tau \right ) ^{T} h _{s-1} \left ( t+1 \right ) \right ]$$&lt;/p&gt;

&lt;p&gt;其中\(A \left ( t \right )\)不需要训练，是随着时间和输入更新的
&lt;strong&gt;Layer normalized fast weight&lt;/strong&gt;
快速联想记忆网络的一个可能潜在的问题是两个隐藏向量的标量积可能会消失或者爆炸，而最近提出的layer normalization在处理这样的问题时表现出色，并且可以减少训练时间，所以我们在快速联想记忆网络中使用这一技术如下：
$$h _{S} \left ( t+1 \right ) = f \left (LN \left [ Wh \left ( t \right ) + Cx \left ( t \right ) + A \left ( t \right ) h _{S-1} \left ( t+1 \right ) \right ] \right ) $$
其中\(LN\left [\cdot \right ] \)代表layer normalization。&lt;/p&gt;

&lt;h3&gt;实验结果&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;联想检索&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/associative-retrieval.png&quot; alt=&quot;associative-retrieval&quot;&gt;
&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/associative-retrieval-result.png&quot; alt=&quot;associative-retrieva-resultl&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在视觉注意力模型中综合瞥见&lt;/p&gt;

&lt;p&gt;视觉注意力模型能将注意力集中在图像的某些重要的部分，而忽略图像的混乱的背景，该模型不仅决定它下一步要看什么，而且要把所看的东西都记住以便进行正确的图像分类。视觉注意力模型能在大量图像输入中找出多个图像并进行正确分类。但是它的瞥见策略过于简单，只能使用单一尺度的瞥见，并且扫描图像的方式也过于死板。相对来说人眼的运动就复杂多了，人眼可以看同一东西的不同尺度，并且能够将看到的东西给予不同的重要程度划分。但是这要求我们能具有短期的记忆能记住我们在刚开始的一系列瞥见中发现了什么。
典型的视觉注意力模型，比较复杂且效果的方差比较大，为了证明快速联想记忆对视觉注意力模型有助益，我们让提前定义好了视觉注意力模型的控制信号，所以模型不需要学习下一步要看什么，这样可以更加明显的看出快速权重的作用。
&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/integrating-glimpses-in-visual-attention-models-result.png&quot; alt=&quot;integrating-glimpses-in-visual-attention-models-result&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;面部表情识别
&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/facial-expression-recognition-result.png&quot; alt=&quot;facial-expression-recognition-result&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;具有记忆的智能体
&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/agents-with-memory-result.png&quot; alt=&quot;agents-with-memory-result&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;扩展阅读&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A-LSTM&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;visual attention models(模型)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Negative log likelihood（评价指标）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;batch normalization(归一化)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;layer normalization(归一化)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Asynchronous advantage actor-critic method(训练方法)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;作者信息&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Jimmy Ba:&lt;/strong&gt;University of Toronto&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using Fast Weights to Attend to the Recent Past (2016)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Learning Wake-Sleep Recurrent Attention Models (2015)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do Deep Nets Really Need to be Deep? (2014)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adaptive dropout for training deep neural networks (2013)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Geoffrey Hinton:&lt;/strong&gt;University of Toronto and Google Brain&lt;/p&gt;

&lt;p&gt;计算机学家和心理学家，反向传播算法和对比散度算法的发明人之一，深度学习的积极推动者。盖茨比计算神经科学中心的创始人，多伦多大学计算机系教授。对神经网络的研究的其它贡献包括Boltzmann机器，分布式表征，时滞神经网络，混合专家、变分学习，专家产品和深信网络。研究兴趣是用丰富传感器输入的非监督式多层神经网络学习程序。可以说是Deep Learning学派的开山祖师爷。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Attend, Infer, Repeat: Fast Scene Understanding with Generative Models (2016)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A Better Way to Pretrain Deep Boltzmann Machines (2012)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ImageNet Classification with Deep Convolutional Neural Networks (2012)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gated Softmax Classification (2010)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Generating more realistic images using gated MRF&amp;#39;s (2010)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Learning to combine foveal glimpses with a third-order Boltzmann machine (2010)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine (2010)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Volodymyr Mnih:&lt;/strong&gt;Google DeepMind&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Learning values across many orders of magnitude (2016)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Strategic Attentive Writer for Learning Macro-Actions (2016)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Recurrent Models of Visual Attention (2014)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Generating more realistic images using gated MRF&amp;#39;s (2010)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Joel Leibo:&lt;/strong&gt;Google DeepMind&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Learning invariant representations and applications to face verification (2013)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Why The Brain Separates Face Recognition From Object Recognition (2011)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Catalin Ionescu:&lt;/strong&gt;Google DeepMind&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Nov 2017 04:21:00 +0800</pubDate>
        <link>http://localhost:4000/mark/paper/2017/11/07/using-fast-weights-to-attend-to-the-recent-past.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/paper/2017/11/07/using-fast-weights-to-attend-to-the-recent-past.html</guid>
        
        <category>nips</category>
        
        <category>2016</category>
        
        
        <category>Paper</category>
        
      </item>
    
      <item>
        <title>1/4</title>
        <description>&lt;p&gt;寂静，叶落风随，嬉笑打闹无情的跨过时间的纵轴敲打着脑海，荡起一阵涟漪。&lt;/p&gt;

&lt;p&gt;风紧呼，三年饮冰热血难凉，佳音何在，与君舞一曲，待重头，能奈若何？&lt;/p&gt;

&lt;p&gt;手心汗未干，时间无法凝固，不过是一场回忆罢了，还是匆匆上车，走吧。&lt;/p&gt;

&lt;p&gt;喊冤？这里没有阎王。&lt;/p&gt;

&lt;p&gt;何去何从，身似柳絮随风飘，好歹挣扎一下吧，风停了，时间却没有&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Oct 2017 19:00:00 +0800</pubDate>
        <link>http://localhost:4000/mark/essay/2017/10/02/quarter-of-life.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/essay/2017/10/02/quarter-of-life.html</guid>
        
        <category>mind</category>
        
        
        <category>Essay</category>
        
      </item>
    
      <item>
        <title>如何在Markdown中添加数学公式</title>
        <description>&lt;h2&gt;三种方式&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;script type=&amp;quot;math/tex; mode=display&amp;quot;&amp;gt;h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;效果：
&lt;script type=&quot;math/tex; mode=display&quot;&gt;h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}&lt;/script&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;$$h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;效果：
$$h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}$$&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;\\(h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}\\)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;效果：
\(h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}\)&lt;/p&gt;
</description>
        <pubDate>Sat, 17 Jun 2017 12:09:00 +0800</pubDate>
        <link>http://localhost:4000/mark/other/2017/06/17/how-to-add-formula-in-markdown.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/other/2017/06/17/how-to-add-formula-in-markdown.html</guid>
        
        <category>markdown</category>
        
        <category>formula</category>
        
        
        <category>Other</category>
        
      </item>
    
  </channel>
</rss>
