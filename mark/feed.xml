<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog</title>
    <description></description>
    <link>http://localhost:4000/mark/</link>
    <atom:link href="http://localhost:4000/mark/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 01 Jun 2018 00:54:14 +0800</pubDate>
    <lastBuildDate>Fri, 01 Jun 2018 00:54:14 +0800</lastBuildDate>
    <generator>Jekyll v3.4.3</generator>
    
      <item>
        <title>Neural Turing Machines (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2018-05-23-neural-turing-machines/2018-05-23-neural-turing-machines.pdf#page=1&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 23 May 2018 21:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/paper/2018/05/23/neural-turing-machines.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/paper/2018/05/23/neural-turing-machines.html</guid>
        
        <category>2014</category>
        
        
        <category>Paper</category>
        
      </item>
    
      <item>
        <title>Machine Learning And Its Application In Image Processing (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2017-06-29-machine-learning-and-its-application-in-image-processing/2017-06-29-machine-learning-and-its-application-in-image-processing.pdf#page=1&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 23 May 2018 21:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/ai/2018/05/23/machine-learning-and-its-application-in-image-processing.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/ai/2018/05/23/machine-learning-and-its-application-in-image-processing.html</guid>
        
        <category>AI</category>
        
        <category>image-processing</category>
        
        
        <category>AI</category>
        
      </item>
    
      <item>
        <title>Ruby On Rails</title>
        <description>&lt;h2&gt;安装&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;安装ruby&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装sqlite3&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install sqlite3
sudo apt-get install libsqlite3-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;安装rails&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;gem install rails
rails --version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用rails创建博客&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;rails new blog
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;启动web服务器&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;cd blog
bin/rails server
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;错误及处理&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:84:in `rescue in block (2 levels) in require&amp;#39;: There was an error while trying to load the gem &amp;#39;uglifier&amp;#39;. (Bundler::GemRequireError)
Gem Load Error is: Could not find a JavaScript runtime. See https://github.com/rails/execjs for a list of available runtimes.
Backtrace for gem load error is:
/var/lib/gems/2.3.0/gems/execjs-2.7.0/lib/execjs/runtimes.rb:58:in `autodetect&amp;#39;
/var/lib/gems/2.3.0/gems/execjs-2.7.0/lib/execjs.rb:5:in `&amp;lt;module:ExecJS&amp;gt;&amp;#39;
/var/lib/gems/2.3.0/gems/execjs-2.7.0/lib/execjs.rb:4:in `&amp;lt;top (required)&amp;gt;&amp;#39;
/var/lib/gems/2.3.0/gems/uglifier-4.1.1/lib/uglifier.rb:5:in `require&amp;#39;
/var/lib/gems/2.3.0/gems/uglifier-4.1.1/lib/uglifier.rb:5:in `&amp;lt;top (required)&amp;gt;&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:81:in `require&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:81:in `block (2 levels) in require&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:76:in `each&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:76:in `block in require&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:65:in `each&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler/runtime.rb:65:in `require&amp;#39;
/var/lib/gems/2.3.0/gems/bundler-1.16.1/lib/bundler.rb:114:in `require&amp;#39;
/home/ez/blog/config/application.rb:7:in `&amp;lt;top (required)&amp;gt;&amp;#39;
/var/lib/gems/2.3.0/gems/railties-5.1.4/lib/rails/commands/server/server_command.rb:133:in `require&amp;#39;
...
...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;原因：没有JavaScript runtime
解决方法：sudo apt-get install nodejs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        <pubDate>Sat, 23 Dec 2017 20:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/web/2017/12/23/begin-ruby-on-rails.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/web/2017/12/23/begin-ruby-on-rails.html</guid>
        
        <category>web ruby ruby-on-rails</category>
        
        
        <category>Web</category>
        
      </item>
    
      <item>
        <title>Playing Atari with Deep Reinforcement Learning (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2017-12-23-playing-atari-with-deep-reinforcement-learning/2017-12-23-playing-atari-with-deep-reinforcement-learning.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Dec 2017 20:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/paper/2017/12/23/playing-atari-with-deep-reinforcement-learning.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/paper/2017/12/23/playing-atari-with-deep-reinforcement-learning.html</guid>
        
        <category>nips</category>
        
        <category>2013</category>
        
        
        <category>Paper</category>
        
      </item>
    
      <item>
        <title>Reinforcement Learning - Markov Decision Process  (pdf)</title>
        <description>&lt;p&gt;&lt;embed src=&quot;/mark/assets/pdf/2017-12-23-markov-decision-process/2017-12-23-markov-decision-process.pdf#page=1&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=800px&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 23 Dec 2017 20:30:00 +0800</pubDate>
        <link>http://localhost:4000/mark/reinforcement-learning/2017/12/23/markov-decision-process.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/reinforcement-learning/2017/12/23/markov-decision-process.html</guid>
        
        <category>MDP</category>
        
        
        <category>Reinforcement-Learning</category>
        
      </item>
    
      <item>
        <title>Using Fast Weights to Attend to the Recent Past</title>
        <description>&lt;h2&gt;论文相关信息&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/by-source-2016-2143&quot;&gt;&lt;strong&gt;论文nips链接&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/6057-using-fast-weights-to-attend-to-the-recent-past.pdf&quot;&gt;&lt;strong&gt;论文pdf链接&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;相关知识&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;记忆分类&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;瞬时记忆，又称为感觉记忆或者感觉登记——也就是你现在看到、听到感觉到的一切信息在人脑中的反应。&lt;/li&gt;
&lt;li&gt;短时记忆——请你现在回忆看这个答案前你再看什么？这就是短时记忆，一般持续15~30秒。（没有复述的情况下）&lt;/li&gt;
&lt;li&gt;长时记忆——也就是一分钟以上的记忆，最长可以达到终身。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上定义来自于《普通心理学》北京师范出版社·彭聃龄&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;循环神经网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/RNN.png&quot; alt=&quot;RNN&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;长短时记忆网络&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/LSTM.png&quot; alt=&quot;LSTM&quot;&gt;&lt;/p&gt;

&lt;h2&gt;论文理解&lt;/h2&gt;

&lt;p&gt;普通的循环神经网络有两种记忆类型，即短期记忆和长期记忆。隐藏层的神经元负责保持短期记忆，每次迭代都更新，记忆容量为\(O(H)\)，其中\(H\)是隐藏层神经元的个数。权重向量保持着长期记忆，记忆容量为\(O(H ^{2})+O(IH)+O(HO)\)，其中\(I\)和\(O\)是输入和输出单元的个数。LSTM增加了各种门和状态信息，使得记忆可以保存的更久，但是它的短期记忆容量依旧为\(O(H)\)。从生理学上来说，人的记忆并不是存储在神经元中。生理学上表明，大脑的短期记忆的某一种实现方式是：通过突出末梢的\([Ca ^{2+}]]\)去极化建立短期记忆，而又通过递质的损耗来遗忘。这也就是说大脑的短期记忆并不是存储于神经元中的，而是存储在突触的变化中。这样短期记忆的容量就可以达到\(O(H ^{2})\)。&lt;/p&gt;

&lt;h3&gt;快速联想记忆&lt;/h3&gt;

&lt;p&gt;1970-1980年的主流思想是，记忆不是以简单的拷贝神经活动的模式存储的，而是在需要时从存有信息的权重中重建的。\(O(N ^{2})\)个权重至多存储\(O(N)\)个实值向量，每个向量有\(O(N)\)个分量。至于实际存储容量能够达到什么程度则取决于我们采用的存储规则。由于我们觉得一个简单的，无迭代的存规则比最大化容量要重要一些，所以我们采用外积规则来存储隐藏快速衰减的神经向量。&lt;/p&gt;

&lt;h4&gt;快速联想记忆的优势：&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;对于像神经图灵机，神经堆栈，记忆网络这些模型暂时没有生理学的依据，但是大脑实现快速联想记忆却是有依据的即通过突触的变化。&lt;/li&gt;
&lt;li&gt;在快速联想记忆中，我们不必决定什么时候读记忆什么时候写记忆，快速联想记忆时时刻刻都在更新。每当输入变化时，都可以通过模型反应到隐藏状态中去。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;模型&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/fast-associative-memory-model.png&quot; alt=&quot;fast-associative-memory-model&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;迭代公式&lt;/strong&gt;
$$A \left ( t \right ) = \lambda  A \left ( t-1 \right ) + \eta h \left ( t \right ) h \left ( t \right ) ^{T}$$&lt;/p&gt;

&lt;p&gt;$$(Assuming\ A\ is\ 0\ at\ the\ beginning\ of\ the\ sequence)$$&lt;/p&gt;

&lt;p&gt;$$\Longrightarrow A \left ( t \right ) = \eta  \sum _{\tau = 1} ^{\tau = t} \lambda ^{t- \tau} h \left ( \tau \right ) h \left ( \tau \right ) ^{T}$$&lt;/p&gt;

&lt;p&gt;$$h _{0} \left ( t+1 \right ) = f \left ( Wh \left ( t \right ) + Cx \left ( t \right ) \right )$$&lt;/p&gt;

&lt;p&gt;$$h _{S} \left ( t+1 \right ) = f \left ( \left [ Wh \left ( t \right ) + Cx \left ( t \right ) \right ] + A \left ( t \right ) h _{S-1} \left ( t+1 \right ) \right ) $$&lt;/p&gt;

&lt;p&gt;$$h \left ( t+1 \right ) = h _{S} \left ( t+1 \right )$$&lt;/p&gt;

&lt;p&gt;$$A \left ( t \right ) h _{s-1} \left ( t+1 \right ) = \eta  \sum _{\tau = 1} ^{\tau = t} \lambda ^{t- \tau} h \left ( \tau \right ) \left [ h \left ( \tau \right ) ^{T} h _{s-1} \left ( t+1 \right ) \right ]$$&lt;/p&gt;

&lt;p&gt;其中\(A \left ( t \right )\)不需要训练，是随着时间和输入更新的
&lt;strong&gt;Layer normalized fast weight&lt;/strong&gt;
快速联想记忆网络的一个可能潜在的问题是两个隐藏向量的标量积可能会消失或者爆炸，而最近提出的layer normalization在处理这样的问题时表现出色，并且可以减少训练时间，所以我们在快速联想记忆网络中使用这一技术如下：
$$h _{S} \left ( t+1 \right ) = f \left (LN \left [ Wh \left ( t \right ) + Cx \left ( t \right ) + A \left ( t \right ) h _{S-1} \left ( t+1 \right ) \right ] \right ) $$
其中\(LN\left [\cdot \right ] \)代表layer normalization。&lt;/p&gt;

&lt;h3&gt;实验结果&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;联想检索&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/associative-retrieval.png&quot; alt=&quot;associative-retrieval&quot;&gt;
&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/associative-retrieval-result.png&quot; alt=&quot;associative-retrieva-resultl&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在视觉注意力模型中综合瞥见&lt;/p&gt;

&lt;p&gt;视觉注意力模型能将注意力集中在图像的某些重要的部分，而忽略图像的混乱的背景，该模型不仅决定它下一步要看什么，而且要把所看的东西都记住以便进行正确的图像分类。视觉注意力模型能在大量图像输入中找出多个图像并进行正确分类。但是它的瞥见策略过于简单，只能使用单一尺度的瞥见，并且扫描图像的方式也过于死板。相对来说人眼的运动就复杂多了，人眼可以看同一东西的不同尺度，并且能够将看到的东西给予不同的重要程度划分。但是这要求我们能具有短期的记忆能记住我们在刚开始的一系列瞥见中发现了什么。
典型的视觉注意力模型，比较复杂且效果的方差比较大，为了证明快速联想记忆对视觉注意力模型有助益，我们让提前定义好了视觉注意力模型的控制信号，所以模型不需要学习下一步要看什么，这样可以更加明显的看出快速权重的作用。
&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/integrating-glimpses-in-visual-attention-models-result.png&quot; alt=&quot;integrating-glimpses-in-visual-attention-models-result&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;面部表情识别
&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/facial-expression-recognition-result.png&quot; alt=&quot;facial-expression-recognition-result&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;具有记忆的智能体
&lt;img src=&quot;/mark/assets/images/2017-11-07-using-fast-weights-to-attend-to-the-recent-Past/agents-with-memory-result.png&quot; alt=&quot;agents-with-memory-result&quot;&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;扩展阅读&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A-LSTM&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;visual attention models(模型)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Negative log likelihood（评价指标）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;batch normalization(归一化)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;layer normalization(归一化)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Asynchronous advantage actor-critic method(训练方法)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;作者信息&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Jimmy Ba:&lt;/strong&gt;University of Toronto&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using Fast Weights to Attend to the Recent Past (2016)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Learning Wake-Sleep Recurrent Attention Models (2015)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do Deep Nets Really Need to be Deep? (2014)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Adaptive dropout for training deep neural networks (2013)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Geoffrey Hinton:&lt;/strong&gt;University of Toronto and Google Brain&lt;/p&gt;

&lt;p&gt;计算机学家和心理学家，反向传播算法和对比散度算法的发明人之一，深度学习的积极推动者。盖茨比计算神经科学中心的创始人，多伦多大学计算机系教授。对神经网络的研究的其它贡献包括Boltzmann机器，分布式表征，时滞神经网络，混合专家、变分学习，专家产品和深信网络。研究兴趣是用丰富传感器输入的非监督式多层神经网络学习程序。可以说是Deep Learning学派的开山祖师爷。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Attend, Infer, Repeat: Fast Scene Understanding with Generative Models (2016)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A Better Way to Pretrain Deep Boltzmann Machines (2012)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ImageNet Classification with Deep Convolutional Neural Networks (2012)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Gated Softmax Classification (2010)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Generating more realistic images using gated MRF&amp;#39;s (2010)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Learning to combine foveal glimpses with a third-order Boltzmann machine (2010)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine (2010)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Volodymyr Mnih:&lt;/strong&gt;Google DeepMind&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Learning values across many orders of magnitude (2016)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Strategic Attentive Writer for Learning Macro-Actions (2016)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Recurrent Models of Visual Attention (2014)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Generating more realistic images using gated MRF&amp;#39;s (2010)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Joel Leibo:&lt;/strong&gt;Google DeepMind&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Learning invariant representations and applications to face verification (2013)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Why The Brain Separates Face Recognition From Object Recognition (2011)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Catalin Ionescu:&lt;/strong&gt;Google DeepMind&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Nov 2017 04:21:00 +0800</pubDate>
        <link>http://localhost:4000/mark/paper/2017/11/07/using-fast-weights-to-attend-to-the-recent-past.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/paper/2017/11/07/using-fast-weights-to-attend-to-the-recent-past.html</guid>
        
        <category>nips</category>
        
        <category>2016</category>
        
        
        <category>Paper</category>
        
      </item>
    
      <item>
        <title>1/4</title>
        <description>&lt;p&gt;寂静，叶落风随，嬉笑打闹无情的跨过时间的纵轴敲打着脑海，荡起一阵涟漪。&lt;/p&gt;

&lt;p&gt;风紧呼，三年饮冰热血难凉，佳音何在，与君舞一曲，待重头，能奈若何？&lt;/p&gt;

&lt;p&gt;手心汗未干，时间无法凝固，不过是一场回忆罢了，还是匆匆上车，走吧。&lt;/p&gt;

&lt;p&gt;喊冤？这里没有阎王。&lt;/p&gt;

&lt;p&gt;何去何从，身似柳絮随风飘，好歹挣扎一下吧，风停了，时间却没有&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Oct 2017 19:00:00 +0800</pubDate>
        <link>http://localhost:4000/mark/essay/2017/10/02/quarter-of-life.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/essay/2017/10/02/quarter-of-life.html</guid>
        
        <category>mind</category>
        
        
        <category>Essay</category>
        
      </item>
    
      <item>
        <title>如何在Markdown中添加数学公式</title>
        <description>&lt;h2&gt;三种方式&lt;/h2&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;script type=&amp;quot;math/tex; mode=display&amp;quot;&amp;gt;h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;效果：
&lt;script type=&quot;math/tex; mode=display&quot;&gt;h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}&lt;/script&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;$$h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}$$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;效果：
$$h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}$$&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&lt;span&gt;&lt;/span&gt;\\(h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}\\)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;效果：
\(h _{\theta }\left ( x \right ) = \theta _{0} + \theta _{1}x _{1} + \theta _{2}x _{2} + ... + \theta _{n}x _{n}\)&lt;/p&gt;
</description>
        <pubDate>Sat, 17 Jun 2017 12:09:00 +0800</pubDate>
        <link>http://localhost:4000/mark/other/2017/06/17/how-to-add-formula-in-markdown.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/other/2017/06/17/how-to-add-formula-in-markdown.html</guid>
        
        <category>markdown</category>
        
        <category>formula</category>
        
        
        <category>Other</category>
        
      </item>
    
      <item>
        <title>Machine Learning - 线性回归</title>
        <description>&lt;h3&gt;假设函数&lt;/h3&gt;

&lt;p&gt;\( h _ {\theta } \left ( x \right ) = \theta _ {0}x _ {0} + \theta _ {1}x _ {1} + \theta _ {2}x _ {2} + ... + \theta _ {n}x _ {n}\)&lt;/p&gt;

&lt;p&gt;这里\( x _{0} \)恒等于1，表示为常数项。&lt;/p&gt;

&lt;h3&gt;损失函数&lt;/h3&gt;

&lt;p&gt;\( J \left ( \theta  \right )= \frac{1}{2m} \sum_{i=1}^{m} \left ( h _{\theta} \left ( x^{\left (i  \right )} \right ) - y^{\left (i  \right )} \right ) ^{2} \)&lt;/p&gt;

&lt;p&gt;其中&lt;code&gt;m&lt;/code&gt;为样本数量&lt;/p&gt;

&lt;h3&gt;目标&lt;/h3&gt;

&lt;p&gt;\( \min J\left ( \theta  \right ) \)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;梯度下降法

&lt;ul&gt;
&lt;li&gt;正统的方法是需要同步更新每个θ&lt;/li&gt;
&lt;li&gt;\( \theta _ {j} = \theta _ {j} - \alpha \frac {1}{m} \sum_{i=1}^{m} \left ( h _ {\theta} \left ( x^{\left (i  \right )} \right ) - y^{\left (i  \right )} \right ) x _{j}^{\left (i  \right )} \)&lt;/br&gt;
其中\( \alpha \) 为学习率&lt;/li&gt;
&lt;li&gt;matlab代码如下&lt;/br&gt;
&lt;code&gt;matlab
theta=theta-(alpha/m*X&amp;#39;*(X*theta-y));
&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;正规方程法&lt;/br&gt;

&lt;ul&gt;
&lt;li&gt;也就是利用数学的方法，根据最小值在导数等于0的地方取得可直接求得结果，但是由于求n×n矩阵的逆运算复杂度为\( O\left ( n^{3} \right ) \),所以当特征很多时不宜用此方法，一般要求n小于10000&lt;/li&gt;
&lt;li&gt;matlab代码如下&lt;/br&gt;
&lt;code&gt;matlab
theta=pinv(X&amp;#39;*X)*X&amp;#39;*y;
&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;当矩阵X&amp;#39;*X的逆不存在时，一般是由于特征值之间具有线性相关性，提示我们重新选取特征值，或者是由于特征值太多而训练样本不足。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 14 Jun 2017 23:06:00 +0800</pubDate>
        <link>http://localhost:4000/mark/machine-learning/2017/06/14/machine-learning-linear-regression.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/machine-learning/2017/06/14/machine-learning-linear-regression.html</guid>
        
        <category>AI</category>
        
        <category>machine-learning</category>
        
        
        <category>Machine-Learning</category>
        
      </item>
    
      <item>
        <title>Linux Kernel - 进程的切换和系统的一般执行过程</title>
        <description>&lt;h3&gt;进程的调度时机与进程的切换&lt;/h3&gt;

&lt;h4&gt;进程调度的时机&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;中断处理过程（包括时钟中断、I/O中断、系统调用和异常）中，直接调用&lt;code&gt;schedule()&lt;/code&gt;,或者返回用户态时根据&lt;code&gt;need_resched&lt;/code&gt;标记调用&lt;code&gt;schedule()&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;内核线程（只有内核态没有用户态的特殊进程）可以直接调用&lt;code&gt;schedule()&lt;/code&gt;进行进程切换，也可以在中断处理过程中进行调度，也就是说内核线程作为一类特殊的进程可以主动调度，也可以被动调度。&lt;/li&gt;
&lt;li&gt;用户态进程无法实现主动调度，仅能通过陷入内核态后的某个时机点进行调度，即在中断处理过程中进行调度。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;进程的切换&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;挂起正在CPU上执行的进程，与中断时保护现场是不同的，中断前后是在同一个进程上下文中，只是由用户态转向内核态执行。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;进程上下文包含了进程执行需要的所有信息&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;用户地址空间：包括程序代码，数据，用户堆栈等。&lt;/li&gt;
&lt;li&gt;控制信息：进程描述符，内核堆栈等。&lt;/li&gt;
&lt;li&gt;硬件上下文（注意中断也要保存硬件上下文只是保存的方法不同）&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://codelab.shiyanlou.com/xref/linux-3.18.6/kernel/sched/core.c#schedule&quot;&gt;&lt;code&gt;schedule()&lt;/code&gt;&lt;/a&gt;函数选择一个新的进程来运行，并调用&lt;a href=&quot;http://codelab.shiyanlou.com/xref/linux-3.18.6/kernel/sched/core.c#context_switch&quot;&gt;&lt;code&gt;context_switch&lt;/code&gt;&lt;/a&gt;进行上下文的切换，其中的宏调用&lt;a href=&quot;http://codelab.shiyanlou.com/xref/linux-3.18.6/arch/x86/include/asm/switch_to.h#switch_to&quot;&gt;&lt;code&gt;switch_to&lt;/code&gt;&lt;/a&gt;进行关键上下文切换。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-C&quot; data-lang=&quot;C&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;asmlinkage&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__visible&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__sched&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_struct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tsk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;sched_submit_work&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tsk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;__schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-C&quot; data-lang=&quot;C&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__sched&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__schedule&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_struct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;switch_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nl&quot;&gt;need_resched&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pick_next_task&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;likely&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;context_switch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* unlocks the rq */&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-C&quot; data-lang=&quot;C&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;context_switch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_struct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_struct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prepare_task_switch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;context_tracking_task_switch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;cm&quot;&gt;/* Here we just switch the register state and the stack. */&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;switch_to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-C&quot; data-lang=&quot;C&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;cp&quot;&gt;#define switch_to(prev, next, last)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;                                                                        \
    &lt;span class=&quot;cm&quot;&gt;/*                                                                      \&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;     * Context-switching clobbers all registers, so we clobber              \&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;     * them explicitly, via unused output variables.                        \&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;     * (EAX and EBP is not listed because EBP is saved/restored             \&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;     * explicitly for wchan access and EAX is the return value of           \&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;     * __switch_to())                                                       \&lt;/span&gt;
&lt;span class=&quot;cm&quot;&gt;     */&lt;/span&gt;                                                                     \
    &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;                                  \
                                                                            \
    &lt;span class=&quot;k&quot;&gt;asm&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;volatile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;                                                           \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;pushfl&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;    &lt;span class=&quot;cm&quot;&gt;/* save    flags */&lt;/span&gt;                             \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;pushl %%ebp&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;    &lt;span class=&quot;cm&quot;&gt;/* save    EBP   */&lt;/span&gt;                        \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;movl %%esp,%[prev_sp]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;  &lt;span class=&quot;cm&quot;&gt;/* save    ESP   */&lt;/span&gt;                \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;movl %[next_sp],%%esp&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;  &lt;span class=&quot;cm&quot;&gt;/* restore ESP   */&lt;/span&gt;                \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;movl $1f,%[prev_ip]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;  &lt;span class=&quot;cm&quot;&gt;/* save    EIP   */&lt;/span&gt;                  \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;pushl %[next_ip]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;  &lt;span class=&quot;cm&quot;&gt;/* restore EIP   */&lt;/span&gt;                     \
            &lt;span class=&quot;n&quot;&gt;__switch_canary&lt;/span&gt;                                                 \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;jmp __switch_to&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;  &lt;span class=&quot;cm&quot;&gt;/* regparm call  */&lt;/span&gt;                        \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;1:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;                                                          \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;popl %%ebp&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;    &lt;span class=&quot;cm&quot;&gt;/* restore EBP   */&lt;/span&gt;                         \
            &lt;span class=&quot;s&quot;&gt;&amp;quot;popfl&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&lt;/span&gt;      &lt;span class=&quot;cm&quot;&gt;/* restore flags */&lt;/span&gt;                              \
                                                                            \
            &lt;span class=&quot;cm&quot;&gt;/* output parameters */&lt;/span&gt;                                         \
            &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;=m&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;                             \
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;=m&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;                             \
              &lt;span class=&quot;s&quot;&gt;&amp;quot;=a&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;                                                  \
                                                                            \
              &lt;span class=&quot;cm&quot;&gt;/* clobbered output registers: */&lt;/span&gt;                             \
              &lt;span class=&quot;s&quot;&gt;&amp;quot;=b&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;=c&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;=d&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;                           \
              &lt;span class=&quot;s&quot;&gt;&amp;quot;=S&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;=D&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                                        \
                                                                            \
              &lt;span class=&quot;n&quot;&gt;__switch_canary_oparam&lt;/span&gt;                                        \
                                                                            \
              &lt;span class=&quot;cm&quot;&gt;/* input parameters: */&lt;/span&gt;                                       \
            &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&amp;quot;m&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;                             \
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&amp;quot;m&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;                             \
                                                                            \
              &lt;span class=&quot;cm&quot;&gt;/* regparm parameters for __switch_to(): */&lt;/span&gt;                   \
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;s&quot;&gt;&amp;quot;a&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;                                        \
              &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;s&quot;&gt;&amp;quot;d&amp;quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                                         \
                                                                            \
              &lt;span class=&quot;n&quot;&gt;__switch_canary_iparam&lt;/span&gt;                                        \
                                                                            \
            &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* reloaded segment registers */&lt;/span&gt;                              \
              &lt;span class=&quot;s&quot;&gt;&amp;quot;memory&amp;quot;&lt;/span&gt;                                                      \
    &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;                                                                      \
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;这里值得注意的是&lt;a href=&quot;http://codelab.shiyanlou.com/xref/linux-3.18.6/arch/x86/include/asm/switch_to.h#switch_to&quot;&gt;&lt;code&gt;switch_to(prev, next, last)&lt;/code&gt;&lt;/a&gt;中的&lt;code&gt;jmp __switch_to&lt;/code&gt;，实际上类似于函数，只不过参数通过寄存器传递，且因为没有&lt;code&gt;call&lt;/code&gt;语句所以不&lt;code&gt;pushl %eip*&lt;/code&gt;(*意在说明这条指令实际不存在，是伪指令)，但是&lt;code&gt;__switch_to&lt;/code&gt;末尾任然有条&lt;code&gt;ret&lt;/code&gt;语句，这样就把之前&lt;code&gt;pushl %[next_ip]&lt;/code&gt;的&lt;code&gt;[next_ip]&lt;/code&gt;&lt;code&gt;popl&lt;/code&gt;给了&lt;code&gt;eip&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Linux系统的一般运行过程。&lt;/h3&gt;

&lt;h4&gt;最一般的情况：正在运行的用户态进程X切换到运行X用户态进程Y的过程&lt;/h4&gt;

&lt;ol&gt;
&lt;li&gt;正在运行的用户态进程X&lt;/li&gt;
&lt;li&gt;发生中断——&lt;code&gt;save cs:eip/ss:esp/eflags(current) to kernel stack, then load cs:eip(entry of specific ISR) and ss:esp(point to kernel stack)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;SAVE_ALL //保存现场&lt;/li&gt;
&lt;li&gt;中断处理过程中或中断返回前调用了schedule()，其中的switch_to做了关键的进程上下文切换&lt;/li&gt;
&lt;li&gt;标号1之后开始开始运行用户态进程Y（这里Y曾经通过以上步骤被切换出去过，因此可以从标号1继续执行）&lt;/li&gt;
&lt;li&gt;restore_all //恢复现场&lt;/li&gt;
&lt;li&gt;&lt;code&gt;iret - pop cs:eip/ss:eip/eflags from kernel stack&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;继续运行用户态进程Y&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;Linux系统执行过程中的几个特殊情况&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;用户态进程与内核态线程之间互相切换，内核态线程不涉及到用户态到内核态的上下文切换，因为内核态线程没有用户态。&lt;/li&gt;
&lt;li&gt;内核态线程之间互相切换。&lt;/li&gt;
&lt;li&gt;内核线程主动调用schedule()（用户态进程不能主动调用schedule()），只有进程上下文的切换，没有发生中断上下文的切换。&lt;/li&gt;
&lt;li&gt;创建子进程的系统调用在子进程中的执行起点及返回用户态，如fork，子进程从&lt;code&gt;ret_from_fork&lt;/code&gt;开始执行，而不是从标号1开始执行。&lt;/li&gt;
&lt;li&gt;加载一个新的可执行程序后返回到用户态的情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;注意&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;所有进程内核态空间是共享的。&lt;/li&gt;
&lt;li&gt;内核态是各中断处理过程和内核线程的集合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;wdk 原创作品转载请注明出处&lt;br&gt;
相关链接 &lt;a href=&quot;http://mooc.study.163.com/course/USTC-1000029000&quot;&gt;《Linux内核分析》MOOC课程http://mooc.study.163.com/course/USTC-1000029000&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 03 Aug 2016 05:16:00 +0800</pubDate>
        <link>http://localhost:4000/mark/linux-3.18.6/2016/08/03/linux-kernel-process-switch-and-system-workflow.html</link>
        <guid isPermaLink="true">http://localhost:4000/mark/linux-3.18.6/2016/08/03/linux-kernel-process-switch-and-system-workflow.html</guid>
        
        <category>linux</category>
        
        <category>linux-kernel</category>
        
        <category>experiment</category>
        
        
        <category>Linux-3.18.6</category>
        
      </item>
    
  </channel>
</rss>
